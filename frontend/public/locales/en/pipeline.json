{
  "pipelines": "Pipelines",
  "basic": "Basic information",
  "id": "ID",
  "name": "Name",
  "desc": "Description",
  "status": "Status",
  "creationMethod": "Creation method",
  "lastEditDate": "Last edit date",
  "tagDesc": "Those tags will propogated to all the resource in this pipeline",
  "valid": {
    "nameEmpty": "Please input pipeline name.",
    "regionEmpty": "Please select region.",
    "vpcEmpty": "Please select VPC.",
    "sdkEmpty": "Please select SDK",
    "s3BucketEmpty": "Please select Amazon S3 Bucket",
    "publicSubnetEmpty": "Please select public subnets",
    "privateSubnetEmpty": "Please select private subnets",
    "domainNameEmpty": "Please input domain name",
    "certificateEmpty": "Please select SSL certificate",
    "dataProcessorIntervalError": "Data processing interval could not be less than 3 minutes",
    "acknowledgeHTTPSecurity": "Please click acknowledge button to continue."
  },
  "create": {
    "basicInfo": "Basic information",
    "configIngestion": "Configure ingestion",
    "dataProcessor": "Configure data processing",
    "reporting": "Reporting",
    "reviewLaunch": "Review and launch",
    "selectFile": "Select a mapping file",
    "selectFileDesc": "Please make sure you use the provided template",
    "selectFileConstraint": "Maximum file size is 25MB",
    "chooseFile": "Choose file",
    "fileSize": "File size in bytes Last date modified",
    "usingUIAlert": "Any field not specified here will be added as an parameter into event_param field.",
    "usingUIInfo": "You can add up to 42 more key.",
    "usingUIAdd": "Add new mapping",
    "dataKey": "Your data key",
    "dataValue": "Data model value",
    "enterKey": "Enter key",
    "enterValue": "Enter value",
    "noItems": "No items associated with the resource.",
    "s3Assets": "Pipeline asset location",
    "s3AssetsDesc": "Choose an Amazon S3 location to store the raw and processed data for this project, you can also specify a storage class for it",
    "selectS3": "Select an Amazon S3 Bucket",
    "kds": {
      "kdsSettings": "Amazon Kinesis Data Stream settings",
      "kdsSettingsDesc": "The solution will create a KDS for you based on your specifications.",
      "shardNum": "Shard number",
      "shardNumDesc": "Specify the number of Shards of the Kinesis Data Streams. Each shard can have up to 1,000 records per second and total data write rate of 1MB per second.",
      "enableAS": "Enable autoscaling",
      "enableASDesc": "Choose Yes if you want to enable autoscaling for the KDS",
      "maxShard": "Maximum shard number",
      "maxShardDesc": "Specify maximum number of shards. ",
      "provisionMode": "Provision mode",
      "provisionModeDesc": "Choose Yes if you want to enable autoscaling for the KDS",
      "selectMode": "Select provision mode",
      "ondemand": "On-demand",
      "provisioned": "Provisioned"
    },
    "msk": {
      "mskCluster": "Connect to an Apache Kafka Cluster",
      "mskClusterDesc": "You can choose to use Amazon Managed Streaming for Apache Kafka (Amazon MSK) or a self-hosted Kafka cluster.",
      "select": "Amazon MSK",
      "topic": "Topic",
      "topicDesc": "By default, the solution will create a topic with “project id”, you can customize it.",
      "enterTopicName": "Enter a custom topic name",
      "manual": "Self-hosted",
      "brokerLink": "Broker link",
      "brokerLinkDesc": "Enter the connection URL for the cluster you wish to connect to.",
      "brokerLindPlaceHolder": "Enter the connection URL",
      "manualTopicDesc": "Please specify the topic for storing the data",
      "selectMSK": "Please select a MKS Cluster",
      "createMSK": "Create a quick-start cluster",
      "createMSKDesc": "The solution creates a small MSK cluster (2vcpu, 8RAM) as starter, suitable for testing purpose",
      "exsitingMSK": "Select an existing cluster",
      "exsitingMSKDesc": "The solution establish connection with a MSK cluster within the same VPC"
    },
    "s3": {
      "selectS3": "Select a S3 bucket for data buffer",
      "selectS3Desc": "Enter a destination in Amazon S3 where your data will be stored. You can consume the data from the selected S3 bucket",
      "s3URI": "S3 URI",
      "s3URIFormat": "Format: s3://bucket.",
      "s3Prefix": "S3 prefix",
      "s3PrefixDesc": "By default, the solution add prefix  of “<project_id>/YYYY/MM/dd/HH” (in UTC) to the data files when it deliver to S3. You can provide additional prefix which will be add before <project_id>. ",
      "s3Constraint": "You can repeat the same keys in your S3 buckeet prefix. Maximum S3 bucket prefix characters: 1024",
      "enterAdditional": "Enter an additional prefix",
      "bufferSize": "Buffer size",
      "bufferSizeDesc": "Specify the data size to buffer before sending to Amazon S3. The higher buffer size may be lower in cost with higher latency, the lower buffer size will be faster in delivery with higher cost. Min: 1 MiB, Max: 50 MiB",
      "bufferInterval": "Buffer interval",
      "bufferIntervalDesc": "The higher interval allows more time to collect data and the size of data may be bigger. The lower interval sends the data more frequently and may be more advantageous when looking at shorter cycles of data activity. Min: 60, Max: 3600"
    },
    "pipeline": "Pipeline",
    "name": "Name",
    "nameDesc": "Please give a name of your  data pipeline that make sense to your organization.",
    "nameConstraint": "The name can be up to 200 characters long. Valid characters are a-z, A-Z, 0-9, . (period), _ (underscore) and - (hyphen).",
    "desc": "Description",
    "descPlaceholder": "your description",
    "awsRegion": "AWS Regions",
    "awsRegionDesc": "Specify the region you want to deploy the pipeline into",
    "awsRegionPlaceholder": "Select an AWS region",
    "vpc": "VPC",
    "vpcDesc": "Specify the VPC you want to deploy the pipeline into",
    "vpcPlaceholder": "Select a VPC",
    "dataSDK": "Data collection SDK",
    "dataSDKDesc": "Specify the SDK you used to send data to this pipeline",
    "dataSDKPlaceholder": "Select a SDK",
    "selectSDKPlaceholder": "Select a SDK",
    "itemSelection": "Items selection",
    "lastEdit": "Last edit",
    "loading": "Loading resources",
    "noPlugin": "No plugins",
    "noPluginDisplay": "No plugins to display.",
    "findPlugin": "Find plugins",
    "selectEnrich": "Select the enrichment plugins that you want to enable or disable",
    "enrichPlugins": "Enrichment plugins",
    "code": "Code",
    "config": "Configuration",
    "enableEdp": "Enable custom endpoint",
    "enableEdpDesc": "The solution provides an auto-generate URL as ingestion endpoint, but you can also use a custom endpoint. Please note, the host name must be registered in Amazon Route53 service",
    "edpSettings": "Ingestion endpoint settings",
    "edpSettingsDesc": "The solution will spin up a web service as an ingestion endpoint to collect data sent from your SDKs.",
    "enableHttps": "Enable HTTPS",
    "domainName": "Domain name",
    "domainNameDesc": "Specifify a domain name for your ingestion endpoint, the solution will create entry in Route53 for you automatically.",
    "domainNameR53Placeholder": "Choose a hostded zone",
    "hostedZone": "Hosted Zone",
    "requestPath": "Request path",
    "requestPathDesc": "The default path for the ingestion endpoint to collect data is “/collect”, you can overwrite it in below textbox.",
    "requestPlaceholder": "collect",
    "ingestionCapacity": "Ingestion capacity",
    "ingestionCapacityDesc": "A single Ingestion Compute Unit (ICU) represents billable compute and memory units, approximately 8 gigabytes (GB) of memory and 2 vCPUs. 1 ICU generally can support 6000 requests per second.",
    "minSize": "Minimun capacity",
    "maxSize": "Maximum capacity",
    "warmPool": "Warm pool",
    "subnet": "Subnet",
    "subnetDesc": "Specifiy the subnets you want the ingestion endpoint to run in",
    "subnetPlaceholder": "Choose the subnet(s)",
    "dataSink": "Data buffer settings",
    "dataSinkDesc": "Configure the how to sink the data for downstream consumption. ",
    "bufferType": "Buffer type",
    "bufferTypeDesc": "Choose the type of data buffer you want to use.",
    "bufferS3": "Amazon S3",
    "bufferS3Desc": "Data is cached in memory then save into a S3 bucket, use this if you can accept the risk of potential data loss.",
    "bufferMSK": "Apache Kafka",
    "bufferMSKDesc": "Data will be buffered into an topic in a Kafka cluster for consumption.",
    "bufferKDS": "Amazon Kinesis Data Stream (KDS)",
    "bufferKDSDesc": "Data will be buffered into Amazon KDS for consumption.",
    "enableModeling": "Enable data modeling",
    "enableModelingDesc": "This solution ships with an inbuilt data models for web and mobile client-side events to create out-of-the-box dashboards in reporting tool, which can also be customized to meet your needs. To enable the data model, you need to provide a mapping between your event data structure and the solution’s data model. If you choose not to use our data model, the raw event data with enrichments (if enabled in step 3) will be stored in the specified data destination without any modeling.",
    "enableDataModel": "Enable data model",
    "modelCreationMethod": "Creation method",
    "modelCreationMethodDesc": "Choose the method to create data mapping",
    "uploadFile": "Upload a mapping file",
    "uploadFileDesc": "Please use the template to define the mapping relationship",
    "usingUI": "Using UI",
    "usingUIDesc": "Define the mapping relastionship for each field on the UI",
    "engineSetting": "Modeling engine settings",
    "engineSettingDesc": "Select the engine to model the data and configure how the data modeling job run",
    "engineRedshift": "Redshift",
    "engineRedshiftDesc": "Data will be loaded into Redshift, modeling job will run by Redshift",
    "engineAthena": "Athena",
    "engineAthenaDesc": "Data will be stored in S3, modeling job will be run by Athena",
    "engineRedshiftCluster": "Redshift cluster",
    "engineRedshiftClusterDesc": "Select a redshift cluster as the data modeling engine for this pipeline ",
    "engineRedshiftClusterPlaceholder": "Find a cluster",
    "engineDataRange": "Data range",
    "engineDataRangeDesc": "Specify the duration for the data you want to load into Redshift for data modeling, Redshift will delete the data beyond specified range. But all data will still be available in S3 for your query with Redshift Specturm.",
    "engineDuration": "Duration",
    "engineDurationPlaceholder": "Enter duration",
    "engineUnitOfTime": "Unit of time",
    "engineDataPartition": "Data partition",
    "engineDataPartitionDesc": "Specify how you want to partition the data in Redshift.",
    "engineDataPartitionAlert": "This setting will impact the dashboard and metrics for your business, please select carefully.",
    "engineBaseIngestionTime": "Partition based on ingestion time",
    "engineBaseIngestionTimeDesc": "Event data will be partitioned based on when it was received by the ingestion endpoint. ",
    "enginebaseEventTime": "Partition based on event time",
    "enginebaseEventTimeDesc": "Event data will be partitioned based the event creation timestamp specified by the client SDK.",
    "reportSettings": "Reporting settings",
    "reportSettingsDesc": "Select a Quicksight account for the solution create reporting for you",
    "createSampleQuickSight": "Create sample dashboard in Quicksight",
    "createSampleQuickSightDesc": "Note that enabling this functionality will allow solution to automatically sign up your AWS account to QuickSight (Enterprise) if your account havn’t signed up yet.",
    "quickSightNotSub": "No QuickSight subscription in your account",
    "quickSightNotSubDesc": "The solution detects that your account has not subscribed to QuickSight, click “Subscribe” button on the right to create subscription. Note additional costs apply.",
    "quickSightUser": "Quicksight user",
    "quickSightUserDesc": "Select a QuickSight user for the solution to create datasets and analyses. Click “Create new” button to create a new user.",
    "quickSIghtPlaceholder": "Select a quicksight user",
    "dataConnectionType": "Data connection type",
    "dataConnectionTypeDesc": "Select a connection type for QuickSight to connect to your data. ",
    "publicNetwork": "Public network",
    "publicNetworkDesc": "Connect to data source over public network",
    "vpcConnection": "VPC connection",
    "vpcConnectionDesc": "Connect to data source through a  VPC connection predefined by QuickSight account admin. Fill in the Name of the VPC connection in below.",
    "ingestSettings": "Ingestion setting",
    "ingestSettingsDesc": "Ingestion setting description",
    "clusterSize": "Cluster Size",
    "topic": "Topic",
    "modelSettings": "Data modeling setting",
    "modelSettingsDesc": "Data modeling setting description",
    "modelEngine": "Modeling engine",
    "enableETL": "Enable data processing ",
    "enableETLDesc1": "This solution ships with an inbuilt data schema ",
    "enableETLDesc2": " to model the raw event data sent from your web and mobile apps, so that it can generate out-of-the-box dashboards and makes it easy for you to build your business-specific analytics. To use our data model, you need to enable ETL to transform the raw event data to our data model. If you choose otherwise, your data will be saved in S3 in its raw format.",
    "transform": "Transform",
    "transformDesc1": "Since you have chosen to use a third-party SDK, you need to provide a custom plugin to transform the raw data to the solution data model ",
    "transformDesc2": " to enable enrichment and data modeling",
    "publicSubnet": "Public Subnets",
    "publicSubnetDesc": "Plase select public subnets",
    "privateSubnet": "Private Subnets",
    "privateSubnetDesc": "Plase select private subnets",
    "workgroup": "Workgroup",
    "workgroupDesc": "Select a work group  to perform the modeling and query",
    "findWorkGroup": "Find a work group",
    "duration": "Duration",
    "executionParam": "Execution parameters",
    "executionParamDesc": "Configure the key behaviors of the data processing jobs.",
    "processInterval": "Data processing interval",
    "processIntervalDesc": "Specify the interval to batch the data for ETL processing. Please make sure the specified interval is longer than the ingestion buffer interval.",
    "eventFreshness": "Event freshness",
    "eventFreshnessDesc": "Event freshness is the period after which the solution will ignore the event data. For example, if you specify 3 days for this parameter, the solution will ignore any event arrive more than 3 days after the events are triggered",
    "anlyEngine": "Analytic engine",
    "anlyEngineDesc": "Select the analytic engine for data modeling and analyzing.",
    "redshift": "Redshift",
    "redshiftAdditionalSettings": "Additional settings",
    "redshiftDesc": "Data will be load into Amazon Redshift data warehouse for further modeling and query.",
    "redshiftCluster": "Redshift cluster",
    "redshiftClusterDesc": "Select a redshift cluster as the data modeling engine for this pipeline.",
    "redshiftBaseCapacity": "Base RPU",
    "redshiftBaseCapacityDesc": "Set the base capacity in Redshift processing units (RPUs) used to process your workload. One RPU provides 16 GB of memory.",
    "redshiftVpc": "VPC",
    "redshiftVpcDesc": "Specify the VPC you want to deploy the Redshift Serverless workgroup into",
    "redshiftSecurityGroup": "Security Group",
    "redshiftSecurityGroupDesc": "This VPC security group defines which subnets and IP ranges can access the Serverless workgroup.",
    "redshiftSecurityGroupPlaceholder": "Choose security group",
    "redshiftSubnet": "Subnet",
    "redshiftSubnetDesc": "This VPC security group defines which subnets and IP ranges can be used in the VPC.",
    "redshiftDatabaseUser": "Database user",
    "redshiftDatabaseUserDesc": "The solution needs permissions to access and create database in Redshift cluster. By default, it grants Redshift Data API with the permissions of the admin user to execute the commands to create DB, tables, and views, as well as loading data.",
    "redshiftDatabaseUserPlaceholder": "database_admin_username",
    "redshiftDataLoadFrequency": "Data load frequency",
    "redshiftDataLoadFrequencyDesc": "Specify the interval for Redshift Data API to load data into Redshift.",
    "redshiftUserTableUpsertFrequency": "User table upsert frequency",
    "redshiftUserTableUpsertFrequencyDesc": "The solution runs a SQL stored procedures to update a user dimension table periodically. Specify the interval to update the user dimension table.",
    "athena": "Athena",
    "athenaDesc": "Select this option will create table schema and sample queries in Athena for you to query the processed data on S3",
    "certificate": "SSL Certificate",
    "selectCertificate": "Choose SSL Certificate",
    "accessPermissions": "Access permissions",
    "accessPermissionsDesc": "The solution needs permissions to access and create database in Redshift Serverless. Create or choose an Identity and Access Management (IAM) role with required permissions. Click Info to see ",
    "permissionLink": "Permissions required to successfully create database in Redshift Serverless",
    "findIAMRole": "Find an IAM role",
    "aga": " AWS Global Accelerator",
    "agaDesc": "Create an accelerator to get static IP addresses that act as a global fixed entry point to your ingestion server, which will improves the availability and performance of your ingestion server. Note that additional charges apply.",
    "auth": "Authentication",
    "authDesc": "You can use OpenID Connector (OIDC) provider to authenticate the request sent to your ingestion server. If you plan to enable it, please create an OIDC client in the OIDC provider then create a secret in AWS Secret Manager with information “issuer”, “token endpoint”, User endpoint”, “Authorization endpoint”, “App client ID”, and “App Client Secret”.",
    "enableALBLog": "Access logs",
    "enableALBLogDesc": "Application Load Balancer support delivering detailed logs of all requests it receives. If you enable this option, the solution will automatically enable access logs for you and store the logs into the S3 bucket you selected in previous step.",
    "connector": "Connector",
    "connectorDesc": "Create a Apache Kafka connector to stream data to S3.",
    "connectorCheck1": "The solution will download the ",
    "s3Connector": "S3 Connector",
    "connectorCheck2": " that has the code for the Lenses Amazon S3 Sink Connector",
    "connectorCheckDesc": "Allow solution to create a custom plugin for the connector.",
    "securityWarning": "Security warning",
    "securityWarningDesc": "Using HTTP protocol is not secure, data will be sent without any encryption, there are high risks of data being leaked or tampered during transmission, please acknowledge the risk before proceed.",
    "nextSteps": "Next steps",
    "nextStepsDesc": "Once you create your domain, use the custom endpoint to create an alias or CNAME mapping in your Domain Name System (DNS) for the custom endpoint.",
    "dataProcessing": "Data processing",
    "secret": "Secret",
    "selectSecret": "Select a secret",
    "createQSSub": "Create QuickSight subscription",
    "qsAccountName": "QuickSight account name",
    "qsAccountNameDesc": "Provide a name for your QuickSight, Note that this name needs to be global unique.",
    "qsUserEmail": "QuickSight user email",
    "qsUserEmailDesc": "Provide a email for your QuickSight.",
    "createQSUser": "Create QuickSight user",
    "qsCreateUserDesc": "provide a link for you to activate the user and change password.",
    "qsUserActive": "Activate user by clicking below link",
    "redshiftServerless": "Serverless",
    "redshiftServerlessDesc": "Low starting cost, auto-scaling capacity.",
    "redshiftProvisioned": "Provisioned",
    "redshiftProvisionedDesc": "Predefined resource capacity for predicable workload"
  },
  "detail": {
    "ingestion": "Ingestion",
    "processing": "Processing",
    "reporting": "Reporting",
    "monitoring": "Monitoring",
    "alarms": "Alarms",
    "publicSubnet": "Public Subnets",
    "privateSubnet": "Private Subnets",
    "ingestionCapacity": "Ingestion capacity",
    "enableHTTPS": "Enable HTTPS",
    "domainName": "Domain name",
    "acm": "ACM",
    "enableAGA": "Enable Global Accelerator",
    "enableAuth": "Enable Authentication",
    "dataBuffer": "Data Buffer",
    "topic": "Topic",
    "enableALBLog": "Enable ALB access log",
    "pipelineID": "pipeline ID",
    "s3Bucket": "Amazon S3 Bucket",
    "sdk": "Data Collection SDK",
    "cfnStack": "Cloudformation Stack",
    "region": "Region",
    "vpc": "VPC",
    "creationTime": "Creation Time",
    "min": "Min",
    "max": "Max",
    "warm": "Warm",
    "status": "Status",
    "dataProcesingInt": "Data processing interval",
    "eventFreshness": "Event freshness",
    "transform": "Transformation",
    "enrichment": "Enrichment",
    "anlyEngine": "Analytic engine",
    "redshiftPermission": "Redshift Permissions",
    "dataRange": "Data range",
    "athena": "Athena",
    "hours": "Hours",
    "minutes": "Minutes",
    "quicksightRole": "QuickSight Role",
    "datasetName": "Dataset name",
    "accessFromConsole": "Access monitoring dashboard in CloudWatch from AWS console.",
    "accessFromConsoleDesc": "The solution had create a CloudWatch Dashboard that includes key metrics for monitoring this data pipeline, click above button to access the dashboard in your AWS console.",
    "accessFromSolution": "Access monitoring dashboard in the solution console.",
    "accessFromSolutionDesc": "The solution had create a CloudWatch dashboard that includes key metrics for monitoring this data pipeline, to integrate the CloudWatch dashboard into the solution console, please follow below steps:",
    "accessFromSolutionStep1": "Go to CloudWatch Settings",
    "accessFromSolutionStep2": "Set up SSO for CloudWatch dashboard sharing",
    "accessFromSolutionStep3": "Go to the Congnito user pool created for CloudWatch dashboard sharing, add the OIDC provider of control plane as the federated OIDC",
    "accessFromSolutionStep4": "Go to CloudWatch Settings, select the federated OIDC as SSO provider",
    "accessFromSolutionStep5": "Go to the clickstream pipeline dashboard, share dashboard to get the sharing URL",
    "accessFromSolutionStep6": "Provide the sharing URL in below input box and click Submit button.",
    "sharingUrl": "Dashboard sharing URL",
    "alarmName": "Alarm name",
    "action": "Action"
  },
  "list": {
    "id": "Pipeline ID",
    "name": "Pipeline name",
    "region": "Region",
    "status": "Status",
    "created": "Creation date",
    "loading": "Loading resources",
    "noPipeline": "No pipelines",
    "noPipelineDisplay": "No pipelines to display.",
    "findPipeline": "Find your pipeline",
    "pipelineDesc": "All the clickstream analytics data pipelines in your AWS account.",
    "pipelineList": "Pipeline List"
  }
}
