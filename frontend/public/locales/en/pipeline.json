{
  "pipelines": "Pipelines",
  "basic": "Basic information",
  "id": "ID",
  "name": "Name",
  "desc": "Description",
  "status": "Status",
  "creationMethod": "Creation method",
  "lastEditDate": "Last edit date",
  "tagDesc": "Those tags will propogated to all the resource in this pipeline",
  "valid": {
    "nameEmpty": "Please input pipeline name.",
    "regionEmpty": "Please select region.",
    "vpcEmpty": "Please select VPC.",
    "sdkEmpty": "Please select SDK",
    "s3BucketEmpty": "Please select Amazon S3 Bucket",
    "publicSubnetEmpty": "Please select public subnets",
    "privateSubnetEmpty": "Please select private subnets",
    "domainNameEmpty": "Please input domain name",
    "certificateEmpty": "Please select SSL certificate"
  },
  "create": {
    "basicInfo": "Basic information",
    "configIngestion": "Configure ingestion",
    "dataProcessor": "Configure data processing",
    "reporting": "Reporting",
    "reviewLaunch": "Review and launch",
    "selectFile": "Select a mapping file",
    "selectFileDesc": "Please make sure you use the provided template",
    "selectFileConstraint": "Maximum file size is 25MB",
    "chooseFile": "Choose file",
    "fileSize": "File size in bytes Last date modified",
    "usingUIAlert": "Any field not specified here will be added as an parameter into event_param field.",
    "usingUIInfo": "You can add up to 42 more key.",
    "usingUIAdd": "Add new mapping",
    "dataKey": "Your data key",
    "dataValue": "Data model value",
    "enterKey": "Enter key",
    "enterValue": "Enter value",
    "noItems": "No items associated with the resource.",
    "s3Assets": "Pipeline asset location",
    "s3AssetsDesc": "Choose an Amazon S3 location to store the raw and processed data for this project, you can also specify a storage class for it",
    "selectS3": "Select an Amazon S3 Bucket",
    "kds": {
      "kdsSettings": "Amazon Kinesis Data Stream settings",
      "kdsSettingsDesc": "The solution will create a KDS for you based on your specifications.",
      "shardNum": "Shard number",
      "shardNumDesc": "Specify the number of Shards of the Kinesis Data Streams. Each shard can have up to 1,000 records per second and total data write rate of 1MB per second.",
      "enableAS": "Enable autoscaling",
      "enableASDesc": "Choose Yes if you want to enable autoscaling for the KDS",
      "maxShard": "Maximum shard number",
      "maxShardDesc": "Specify maximum number of shards. ",
      "provisionMode": "Provision mode",
      "provisionModeDesc": "Choose Yes if you want to enable autoscaling for the KDS",
      "selectMode": "Select provision mode",
      "ondemand": "On-demand",
      "provisioned": "Provisioned"
    },
    "msk": {
      "mskCluster": "Connect to an Apache Kafka Cluster",
      "mskClusterDesc": "You can choose to use Amazon Managed Streaming for Apache Kafka (Amazon MSK) or a self-hosted Kafka cluster.",
      "select": "Amazon MSK",
      "topic": "Topic",
      "topicDesc": "By default, the solution will create a topic with “project id”, you can customize it.",
      "enterTopicName": "Enter a custom topic name",
      "manual": "Self-hosted",
      "brokerLink": "Broker link",
      "brokerLinkDesc": "Enter the connection URL for the cluster you wish to connect to.",
      "brokerLindPlaceHolder": "Enter the connection URL",
      "manualTopicDesc": "Please specify the topic for storing the data",
      "selectMSK": "Please select a MKS Cluster",
      "createMSK": "Create a quick-start cluster",
      "createMSKDesc": "The solution creates a small MSK cluster (2vcpu, 8RAM) as starter, suitable for testing purpose",
      "exsitingMSK": "Select an existing cluster",
      "exsitingMSKDesc": "The solution establish connection with a MSK cluster within the same VPC"
    },
    "s3": {
      "selectS3": "Select a S3 bucket for data buffer",
      "selectS3Desc": "Enter a destination in Amazon S3 where your data will be stored. You can consume the data from the selected S3 bucket",
      "s3URI": "S3 URI",
      "s3URIFormat": "Format: s3://bucket.",
      "s3Prefix": "S3 prefix",
      "s3PrefixDesc": "By default, the solution add prefix  of “<project_id>/YYYY/MM/dd/HH” (in UTC) to the data files when it deliver to S3. You can provide additional prefix which will be add before <project_id>. ",
      "s3Constraint": "You can repeat the same keys in your S3 buckeet prefix. Maximum S3 bucket prefix characters: 1024",
      "enterAdditional": "Enter an additional prefix",
      "bufferSize": "Buffer size",
      "bufferSizeDesc": "Specify the data size to buffer before sending to Amazon S3. The higher buffer size may be lower in cost with higher latency, the lower buffer size will be faster in delivery with higher cost. Min: 1 MiB, Max: 50 MiB",
      "bufferInterval": "Buffer interval",
      "bufferIntervalDesc": "The higher interval allows more time to collect data and the size of data may be bigger. The lower interval sends the data more frequently and may be more advantageous when looking at shorter cycles of data activity. Min: 60, Max: 3600"
    },
    "pipeline": "Pipeline",
    "name": "Name",
    "nameDesc": "Please give a name of your  data pipeline that make sense to your organization.",
    "nameConstraint": "The name can be up to 200 characters long. Valid characters are a-z, A-Z, 0-9, . (period), _ (underscore) and - (hyphen).",
    "desc": "Description",
    "descPlaceholder": "your description",
    "awsRegion": "AWS Regions",
    "awsRegionDesc": "Specify the region you want to deploy the pipeline into",
    "awsRegionPlaceholder": "Select an AWS region",
    "vpc": "VPC",
    "vpcDesc": "Specify the VPC you want to deploy the pipeline into",
    "vpcPlaceholder": "Select a VPC",
    "dataSDK": "Data collection SDK",
    "dataSDKDesc": "Specify the SDK you used to send data to this pipeline",
    "dataSDKPlaceholder": "Select a SDK",
    "selectSDKPlaceholder": "Select a SDK",
    "itemSelection": "Items selection",
    "lastEdit": "Last edit",
    "loading": "Loading resources",
    "noPlugin": "No plugins",
    "noPluginDisplay": "No plugins to display.",
    "findPlugin": "Find plugins",
    "selectEnrich": "Select the enrichment plugins that you want to enable or disable",
    "enrichPlugins": "Enrichment plugins",
    "code": "Code",
    "config": "Configuration",
    "enableEdp": "Enable custom endpoint",
    "enableEdpDesc": "The solution provides an auto-generate URL as ingestion endpoint, but you can also use a custom endpoint. Please note, the host name must be registered in Amazon Route53 service",
    "edpSettings": "Ingestion endpoint settings",
    "edpSettingsDesc": "The solution will spin up a web service as an ingestion endpoint to collect data sent from your SDKs.",
    "enableHttps": "Enable HTTPS",
    "domainName": "Domain name",
    "domainNameDesc": "Specifify a domain name for your ingestion endpoint, the solution will create entry in Route53 for you automatically.",
    "domainNameR53Placeholder": "Choose a hostded zone",
    "hostedZone": "Hosted Zone",
    "requestPath": "Request path",
    "requestPathDesc": "The default path for the ingestion endpoint to collect data is “/collect”, you can overwrite it in below textbox.",
    "requestPlaceholder": "collect",
    "ingestionCapacity": "Ingestion capacity",
    "ingestionCapacityDesc": "A single Ingestion Compute Unit (ICU) represents billable compute and memory units, approximately 8 gigabytes (GB) of memory and 2 vCPUs. 1 ICU generally can support 6000 requests per second.",
    "minSize": "Minimun capacity",
    "maxSize": "Maximum capacity",
    "warmPool": "Warm pool",
    "subnet": "Subnet",
    "subnetDesc": "Specifiy the subnets you want the ingestion endpoint to run in",
    "subnetPlaceholder": "Choose the subnet(s)",
    "dataSink": "Data buffer settings",
    "dataSinkDesc": "Configure the how to sink the data for downstream consumption. ",
    "bufferType": "Buffer type",
    "bufferTypeDesc": "Choose the type of data buffer you want to use.",
    "bufferS3": "Amazon S3",
    "bufferS3Desc": "Data is cached in memory then save into a S3 bucket, use this if you can accept the risk of potential data loss.",
    "bufferMSK": "Apache Kafka",
    "bufferMSKDesc": "Data will be buffered into an topic in a Kafka cluster for consumption.",
    "bufferKDS": "Amazon Kinesis Data Stream (KDS)",
    "bufferKDSDesc": "Data will be buffered into Amazon KDS for consumption.",
    "enableModeling": "Enable data modeling",
    "enableModelingDesc": "This solution ships with an inbuilt data models for web and mobile client-side events to create out-of-the-box dashboards in reporting tool, which can also be customized to meet your needs. To enable the data model, you need to provide a mapping between your event data structure and the solution’s data model. If you choose not to use our data model, the raw event data with enrichments (if enabled in step 3) will be stored in the specified data destination without any modeling.",
    "enableDataModel": "Enable data model",
    "modelCreationMethod": "Creation method",
    "modelCreationMethodDesc": "Choose the method to create data mapping",
    "uploadFile": "Upload a mapping file",
    "uploadFileDesc": "Please use the template to define the mapping relationship",
    "usingUI": "Using UI",
    "usingUIDesc": "Define the mapping relastionship for each field on the UI",
    "engineSetting": "Modeling engine settings",
    "engineSettingDesc": "Select the engine to model the data and configure how the data modeling job run",
    "engineRedshift": "Redshift",
    "engineRedshiftDesc": "Data will be loaded into Redshift, modeling job will run by Redshift",
    "engineAthena": "Athena",
    "engineAthenaDesc": "Data will be stored in S3, modeling job will be run by Athena",
    "engineRedshiftCluster": "Redshift cluster",
    "engineRedshiftClusterDesc": "Select a redshift cluster as the data modeling engine for this pipeline ",
    "engineRedshiftClusterPlaceholder": "Find a cluster",
    "engineDataRange": "Data range",
    "engineDataRangeDesc": "Specify the duration for the data you want to load into Redshift for data modeling, Redshift will delete the data beyond specified range. But all data will still be available in S3 for your query with Redshift Specturm.",
    "engineDuration": "Duration",
    "engineDurationPlaceholder": "Enter duration",
    "engineUnitOfTime": "Unit of time",
    "engineDataPartition": "Data partition",
    "engineDataPartitionDesc": "Specify how you want to partition the data in Redshift.",
    "engineDataPartitionAlert": "This setting will impact the dashboard and metrics for your business, please select carefully.",
    "engineBaseIngestionTime": "Partition based on ingestion time",
    "engineBaseIngestionTimeDesc": "Event data will be partitioned based on when it was received by the ingestion endpoint. ",
    "enginebaseEventTime": "Partition based on event time",
    "enginebaseEventTimeDesc": "Event data will be partitioned based the event creation timestamp specified by the client SDK.",
    "reportSettings": "Reporting settings",
    "reportSettingsDesc": "Select a Quicksight account for the solution create reporting for you",
    "createSampleQuickSight": "Create sample dashboard in Quicksight",
    "quickSightAccount": "Quicksight Role",
    "quickSightAccountDesc": "Select a Quicksight account to create dashbaord",
    "quickSIghtPlaceholder": "Find a quicksight",
    "datasetName": "Dataset name",
    "datasetNameDesc": "Please provide a name for the dataset in  the Quicksight",
    "ingestSettings": "Ingestion setting",
    "ingestSettingsDesc": "Ingestion setting description",
    "clusterSize": "Cluster Size",
    "topic": "Topic",
    "modelSettings": "Data modeling setting",
    "modelSettingsDesc": "Data modeling setting description",
    "modelEngine": "Modeling engine",
    "enableETL": "Enable data processing ",
    "enableETLDesc1": "This solution ships with an inbuilt data schema ",
    "enableETLDesc2": " to model the raw event data sent from your web and mobile apps, so that it can generate out-of-the-box dashboards and makes it easy for you to build your business-specific analytics. To use our data model, you need to enable ETL to transform the raw event data to our data model. If you choose otherwise, your data will be saved in S3 in its raw format.",
    "transform": "Transform",
    "transformDesc1": "Since you have chosen to use a third-party SDK, you need to provide a custom plugin to transform the raw data to the solution data model ",
    "transformDesc2": " to enable enrichment and data modeling",
    "publicSubnet": "Public Subnets",
    "publicSubnetDesc": "Plase select public subnets",
    "privateSubnet": "Private Subnets",
    "privateSubnetDesc": "Plase select private subnets",
    "workgroup": "Workgroup",
    "workgroupDesc": "Select a work group  to perform the modeling and query",
    "findWorkGroup": "Find a work group",
    "duration": "Duration",
    "executionParam": "Execution parameters",
    "executionParamDesc": "Configure the key behaviors of the data processing jobs.",
    "processInterval": "Data processing interval",
    "processIntervalDesc": "Specify the interval to batch the data for ETL processing. Please make sure the specified interval is longer than the ingestion buffer interval.",
    "eventFreshness": "Event freshness",
    "eventFreshnessDesc": "Event freshness is the period after which the solution will ignore the event data. For example, if you specify 3 days for this parameter, the solution will ignore any event arrive more than 3 days after the events are triggered",
    "anlyEngine": "Analytic engine",
    "anlyEngineDesc": "Select the analytic engine for data modeling and analyzing.",
    "redshift": "Redshift",
    "redshiftDesc": "Data will be load into Amazon Redshift data warehouse for further modeling and query.",
    "redshiftCluster": "Redshift cluster",
    "redshiftClusterDesc": "Select a redshift cluster as the data modeling engine for this pipeline.",
    "athena": "Athena",
    "athenaDesc": "Select this option will create table schema and sample queries in Athena for you to query the processed data on S3",
    "certificate": "SSL Certificate",
    "selectCertificate": "Choose SSL Certificate"
  },
  "detail": {
    "ingestionEdp": "Ingestion endpoint",
    "enrichment": "Enrichment",
    "dataModeling": "Data modeling"
  },
  "list": {
    "id": "Pipeline ID",
    "name": "Pipeline name",
    "region": "Region",
    "status": "Status",
    "created": "Creation date",
    "loading": "Loading resources",
    "noPipeline": "No pipelines",
    "noPipelineDisplay": "No pipelines to display.",
    "findPipeline": "Find your pipeline",
    "pipelineDesc": "All the clickstream analytics data pipelines in your AWS account.",
    "pipelineList": "Pipeline List"
  }
}
