{
  "pipelines": "Pipelines",
  "basic": "Basic information",
  "name": "Name",
  "desc": "Description",
  "creationMethod": "Creation method",
  "lastEditDate": "Last edit date",
  "tagDesc": "Those tags will propogated to all the resource in this pipeline",
  "create": {
    "basicInfo": "Basic information",
    "configIngestion": "Configure ingestion",
    "configEnrich": "Configure enrichment",
    "dataModeling": "Data modeling",
    "reviewLaunch": "Review and launch",
    "selectFile": "Select a mapping file",
    "selectFileDesc": "Please make sure you use the provided template",
    "selectFileConstraint": "Maximum file size is 25MB",
    "chooseFile": "Choose file",
    "fileSize": "File size in bytes Last date modified",
    "usingUIAlert": "Any field not specified here will be added as an parameter into event_param field.",
    "usingUIInfo": "You can add up to 42 more key.",
    "usingUIAdd": "Add new mapping",
    "dataKey": "Your data key",
    "dataValue": "Data model value",
    "enterKey": "Enter key",
    "enterValue": "Enter value",
    "noItems": "No items associated with the resource.",
    "kds": {
      "kdsSettings": "Amazon Kinesis Data Stream settings",
      "kdsSettingsDesc": "The solution will create a KDS for you based on your specifications.",
      "shardNum": "Shard number",
      "shardNumDesc": "Specify the number of Shards of the Kinesis Data Streams. Each shard can have up to 1,000 records per second  and total data write rate of 1MB per second. (* Kinesis shard adjustment limit per 24 hours)",
      "enableAS": "Enable autoscaling",
      "enableASDesc": "Choose Yes if you want to enable autoscaling for the KDS",
      "maxShard": "Maximum shard number",
      "maxShardDesc": "Specify maximum number of shards. "
    },
    "msk": {
      "mskCluster": "Select an Amazon MSK Cluster for data sink",
      "mskClusterDesc": "Select a MSK  where your data will be stored. You can consume the data from the specified topic within the selected MSK",
      "select": "Select",
      "topic": "Topic",
      "topicDesc": "By default, the solution will create a topic with “project id”, you can customize it.",
      "enterTopicName": "Enter a custom topic name",
      "manual": "Manual",
      "toBeDesign": "To be design"
    },
    "s3": {
      "selectS3": "Select a S3 bucket for data sink",
      "selectS3Desc": "Enter a destination in Amazon S3 where your data will be stored. You can consume the data from the selected S3 bucket",
      "s3URI": "S3 URI",
      "s3URIFormat": "Format: s3://bucket/prefix.",
      "s3Prefix": "S3",
      "s3PrefixDesc": "By default, the solution add prefix  of “<project_id>/YYYY/MM/dd/HH” (in UTC) to the data files when it deliver to S3. You can provide additional prefix which will be add before <project_id>. ",
      "s3Constraint": "You can repeat the same keys in your S3 buckeet prefix. Maximum S3 bucket prefix characters: 1024",
      "enterAdditional": "Enter an additional prefix"
    },
    "pipeline": "Pipeline",
    "name": "Name",
    "nameDesc": "Please give a name of your  data pipeline that make sense to your organization.",
    "nameConstraint": "The name can be up to 200 characters long. Valid characters are a-z, A-Z, 0-9, . (period), _ (underscore) and - (hyphen).",
    "desc": "Description",
    "descPlaceholder": "your description",
    "awsRegion": "AWS Regions",
    "awsRegionDesc": "Specify the region you want to deploy the pipeline into",
    "awsRegionPlaceholder": "Select an AWS region",
    "vpc": "VPC",
    "vpcDesc": "Specify the VPC you want to deploy the pipeline into",
    "vpcPlaceholder": "Select a VPC",
    "dataSDK": "Data collection SDK",
    "dataSDKDesc": "Specify the SDK you used to send data to this pipeline",
    "dataSDKPlaceholder": "Select a SDK",
    "creationMethod": "Pipeline creation method",
    "creationMethodDesc": "You can create pipeline following the wizard or using a template.",
    "followingWizard": "Following the wizard",
    "followingWizardDesc": "Wizard description",
    "usingTemplate": "Create using a template",
    "usingTemplateDesc": "Template description",
    "selectSDKPlaceholder": "Select a SDK",
    "itemSelection": "Items selection",
    "lastEdit": "Last edit",
    "loading": "Loading resources",
    "noPlugin": "No plugins",
    "noPluginDisplay": "No plugins to display.",
    "findPlugin": "Find plugins",
    "selectEnrich": "Select the enrichment plugins that you want to enable or disable",
    "enrichPlugins": "Enrichment plugins",
    "code": "Code",
    "config": "Configuration",
    "edpSettings": "Ingestion endpoint settings",
    "edpSettingsDesc": "The solution will spin up a web service as an ingestion endpoint to collect data sent from your SDKs.",
    "deployMode": "Deployment mode",
    "deployModeDesc": "Specify how you want to deploy the web service.",
    "serverBase": "Server-based (ECS)",
    "serverBaseDesc": "An ECS task running Nginx and Vector sevices with autoscaling. Use this when your data RPS and the throughput are predicatable.",
    "serverless": "Serverless",
    "serverlessDesc": "An API Gateway endpoint that forward request to KInesis directly. Use this mode when your data RPS and throughput are unpredicable ",
    "ecsSize": "ECS cluster size",
    "ecsSizeDesc": "Select a cluster size based on your data throughput",
    "enableHttps": "Enable HTTPS",
    "domainName": "Domain name",
    "domainNameDesc": "Specifify a domain name for your ingestion endpoint, the solution will create entry in Route53 for you automatically.",
    "domainNamePlaceholder": "custom domain name",
    "domainNameR53Placeholder": "Select a hostzone in R53",
    "sslName": "SSL/TLS Certificate",
    "sslNameDesc": "import a certificate with AWS Certificate Manager (ACM)",
    "chooseCertPlaceholder": "Choose a certificate",
    "requestPath": "Request path",
    "requestPathDesc": "The default path for the ingestion endpoint to collect data is “/collect”, you can overwrite it in below textbox.",
    "requestPlaceholder": "collect",
    "asName": "Autoscaling",
    "asNameDesc": "Specify how you want to autoscale the ECS Cluster",
    "minSize": "Minimun size",
    "maxSize": "Maximum size",
    "warmPool": "Warm pool",
    "subnet": "Subnet",
    "subnetDesc": "Specifiy the subnets you want the ingestion endpoint to run in",
    "subnetPlaceholder": "Choose the subnet(s)",
    "dataSink": "Data sink settings",
    "dataSinkDesc": "Configure the how to sink the data for downstream consumption. ",
    "sinkType": "Sink type",
    "sinkTypeDesc": "Choose the type of data sink you want to use.",
    "sinkS3": "S3",
    "sinkS3Desc": "Data will be sinked into a S3 bucket directly, use this if you do not need to stream data.",
    "sinkMSK": "Amazon MSK (Kalfka)",
    "sinkMSKDesc": "Data will be sinked into an topic in Amazon MSK and then stream into enrichment module",
    "sinkKDS": "Amazon KDS (Kinesis Data Stream)",
    "sinkKDSDesc": "Data will be sinked into an topic in Amazon KDS and then stream into enrichment module",
    "enableModeling": "Enable data modeling",
    "enableModelingDesc": "This solution ships with an inbuilt data models for web and mobile client-side events to create out-of-the-box dashboards in reporting tool, which can also be customized to meet your needs. To enable the data model, you need to provide a mapping between your event data structure and the solution’s data model. If you choose not to use our data model, the raw event data with enrichments (if enabled in step 3) will be stored in the specified data destination without any modeling.",
    "enableDataModel": "Enable data model",
    "modelCreationMethod": "Creation method",
    "modelCreationMethodDesc": "Choose the method to create data mapping",
    "uploadFile": "Upload a mapping file",
    "uploadFileDesc": "Please use the template to define the mapping relationship",
    "usingUI": "Using UI",
    "usingUIDesc": "Define the mapping relastionship for each field on the UI",
    "engineSetting": "Modeling engine settings",
    "engineSettingDesc": "Select the engine to model the data and configure how the data modeling job run",
    "engineRedshift": "Redshift",
    "engineRedshiftDesc": "Data will be loaded into Redshift, modeling job will run by Redshift",
    "engineAthena": "Athena",
    "engineAthenaDesc": "Data will be stored in S3, modeling job will be run by Athena",
    "engineRedshiftCluster": "Redshift cluster",
    "engineRedshiftClusterDesc": "Select a redshift cluster as the data modeling engine for this pipeline ",
    "engineRedshiftClusterPlaceholder": "Find a cluster",
    "engineDataRange": "Data range",
    "engineDataRangeDesc": "Specify the duration for the data you want to load into Redshift for data modeling, Redshift will delete the data beyond specified range. But all data will still be available in S3 for your query with Redshift Specturm.",
    "engineDuration": "Duration",
    "engineDurationPlaceholder": "Enter duration",
    "engineUnitOfTime": "Unit of time",
    "engineDataPartition": "Data partition",
    "engineDataPartitionDesc": "Specify how you want to partition the data in Redshift.",
    "engineDataPartitionAlert": "This setting will impact the dashboard and metrics for your business, please select carefully.",
    "engineBaseIngestionTime": "Partition based on ingestion time",
    "engineBaseIngestionTimeDesc": "Event data will be partitioned based on when it was received by the ingestion endpoint. ",
    "enginebaseEventTime": "Partition based on event time",
    "enginebaseEventTimeDesc": "Event data will be partitioned based the event creation timestamp specified by the client SDK.",
    "reportSettings": "Reporting settings",
    "reportSettingsDesc": "Select a Quicksight account for the solution create reporting for you",
    "createSampleQuickSight": "Create sample dashboard in Quicksight",
    "quickSightAccount": "Quicksight account",
    "quickSightAccountDesc": "Select a Quicksight account to create dashbaord",
    "quickSIghtPlaceholder": "Find a quicksight",
    "datasetName": "Dataset name",
    "datasetNameDesc": "Please provide a name for the dataset in  the Quicksight",
    "ingestSettings": "Ingestion setting",
    "ingestSettingsDesc": "Ingestion setting description",
    "clusterSize": "Cluster Size",
    "topic": "Topic",
    "modelSettings": "Data modeling setting",
    "modelSettingsDesc": "Data modeling setting description",
    "modelEngine": "Modeling engine"
  },
  "detail": {
    "ingestionEdp": "Ingestion endpoint",
    "enrichment": "Enrichment",
    "dataModeling": "Data modeling"
  },
  "list": {
    "id": "Pipeline ID",
    "name": "Pipeline name",
    "region": "Region",
    "status": "Status",
    "created": "Creation date",
    "loading": "Loading resources",
    "noPipeline": "No pipelines",
    "noPipelineDisplay": "No pipelines to display.",
    "findPipeline": "Find your pipeline",
    "pipelineDesc": "All the clickstream analytics data pipelines in your AWS account.",
    "pipelineList": "Pipeline List"
  }
}
